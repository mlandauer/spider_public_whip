#!/usr/bin/env ruby

require 'mechanize'
require 'active_record'
require 'sqlite3'
require 'csv'

def get_page_links(url)
  agent = Mechanize.new
  base = URI.parse(url)
  # Ignore links with nil urls
  begin
    result = agent.get(url)
  rescue
    puts "WARNING: Error retrieving page #{url}"
    return []
  end
  result.links.map{|link| link.href}.compact.map do |link|
    u = base + link
    # Remove anchors 
    u.fragment = nil
    # A couple of special query parameters that we will sanitize because they result in massive duplication
    # of urls in a not particularly helpful way. Both of these parameters simply specify a return url
    u.query = u.query.split("&").delete_if do |t|
      parameter = t.split("=").first
      parameter == "r" || parameter == "rr"
    end.join("&") if u.query
    u.to_s
  end
end

def db_connect(db)
  ActiveRecord::Base.establish_connection(
    adapter:  'sqlite3', # or 'postgresql' or 'sqlite3'
    database: db
  )
end

class CreateLinks < ActiveRecord::Migration
  def self.up
    create_table :links do |t|
      t.string :url, null: false
      t.string :source_url
      # Whether to follow the links on this page
      t.boolean :spider, null: false
      t.boolean :done, null: false, default: false
      t.timestamps
    end

    add_index :links, [:spider, :done]
    add_index :links, :url
  end
end

class Link < ActiveRecord::Base
end

db = "links.sqlite3"
csv = "links.csv"

if File.exists?(db) 
  db_connect(db)
else
  db_connect(db)
  # Create the tables we need
  CreateLinks.up
  # And put the first url in it
  Link.create!(url: "http://localhost", spider: true, done: false)
end

while !Link.where(spider: true, done: false).empty?
  link = Link.where(spider: true, done: false).first
  puts "#{link.url} (#{Link.where(spider: true, done: false).count} left)"
  new_urls = get_page_links(link.url)
  ActiveRecord::Base.transaction do
    new_urls.each do |url|
      # So let's spider this one (unless we're already doing it)
      if Link.find_by(url: url).nil?
        Link.create(url: url, source_url: link.url, spider: (URI.parse(url).host == "localhost"))
      end
    end
    link.update_attributes(done: true)
  end
end

puts "Writing CSV file #{csv}..."
# Write result to CSV file
CSV.open(csv, "w") do |csv|
  csv << ["Source URL", "URL (Host)", "URL (Path)", "URL (queries)"]
  # For the time being don't output external urls
  Link.where(spider: true).order(:url).each do |link|
    u = URI.parse(link.url)
    queries = (u.query ? u.query.split("&").map{|t| t.split("=")}.flatten : []).map{|t| CGI::unescape(t)}
    csv << [link.source_url, u.host, u.path] + queries
  end
end

