#!/usr/bin/env ruby

require 'mechanize'
require 'active_record'
require 'sqlite3'
require 'csv'

def get_page_links(agent, url)
  base = URI.parse(url)
  # Ignore links with nil urls
  begin
    result = agent.get(url)
  rescue
    puts "WARNING: Error retrieving page #{url}"
    return nil
  end
  # We don't want to follow any logout links
  links = result.links.delete_if{|link| link.text == "Logout"}
  links.map{|link| link.href}.compact.map do |link|
    u = base + link
    # Remove anchors
    u.fragment = nil
    # A couple of special query parameters that we will sanitize because they result in massive duplication
    # of urls in a not particularly helpful way. Both of these parameters simply specify a return url
    # Also remove comparison urls between two people. This is to avoid the number of urls blowing up to n**2
    u.query = u.query.split("&").delete_if do |t|
      parameter = t.split("=").first
      parameter == "r" || parameter == "rr"
    end.join("&") if u.query
    u.to_s
  end
end

def db_connect(db)
  ActiveRecord::Base.establish_connection(
    adapter:  'sqlite3', # or 'postgresql' or 'sqlite3'
    database: db
  )
end

class CreateLinks < ActiveRecord::Migration
  def self.up
    create_table :links do |t|
      t.string :url, null: false
      t.string :source_url
      # Whether to follow the links on this page
      t.boolean :spider, null: false
      t.boolean :done, null: false, default: false
      t.boolean :error, null: false, default: false
      t.boolean :comparison_url, null: false, default: false
      t.timestamps
    end

    add_index :links, [:spider, :done]
    add_index :links, :url
    add_index :links, :comparison_url
  end
end

class Link < ActiveRecord::Base
end

def comparison_url?(url)
  q = URI.parse(url).query
  q ? q.split("&").any?{|q| q =~ /(mpn2|mpc2|house2)=/} : false
end

db = "links.sqlite3"
csv = "links.csv"

if File.exists?(db)
  db_connect(db)
else
  db_connect(db)
  # Create the tables we need
  CreateLinks.up
  # And put the first url in it
  Link.create!(url: "http://localhost:8080", spider: true, done: false)
end

agent = Mechanize.new
# First login
#page = agent.get("http://localhost/account/settings.php")
#form = page.forms.first
#form["user_name"] = "mlandauer"
#form["password"] = "foofoo"
#form.click_button

while !Link.where(spider: true, done: false).empty?
  link = Link.where(spider: true, done: false).first
  puts "#{link.url} (#{Link.where(spider: true, done: false).count} left)"
  new_urls = get_page_links(agent, link.url)
  if new_urls.nil?
    # There was an error reading from this url
    link.update_attributes(done: true, error: true)
  else
    ActiveRecord::Base.transaction do
      new_urls.each do |url|
        # So let's spider this one (unless we're already doing it)
        if Link.find_by(url: url).nil?
          # Check if the url is for one of the people comparison pages. If so handle it in a special way
          if comparison_url?(url)
            comparison_count = Link.where(comparison_url: true).count
            if comparison_count < 1000
              puts "Accepting comparison url: #{url} (#{comparison_count})"
              Link.create(url: url, source_url: link.url, spider: (URI.parse(url).host == "localhost"), comparison_url: true)
            end
          else
            Link.create(url: url, source_url: link.url, spider: (URI.parse(url).host == "localhost"))
          end
        end
      end
      link.update_attributes(done: true)
    end
  end
end

# Trace back where a url came from
def show_url_source(link)
  while link.source_url
    puts "*** #{link.url}"
    link = Link.find_by_url(link.source_url)
  end
end

# Extract the query parameters from the url
def query_params(link)
  url = link.url
  u = URI.parse(url)
  queries = {}
  if u.query
    u.query.split("&").each do |t|
      key, value = t.split("=").map{|t| CGI::unescape(t)}
      if queries.has_key?(key)
        puts "WARNING: URL with duplicate params: #{url}"
        show_url_source(link)
      end
      queries[key] = value
    end
  end
  queries
end

puts "Writing CSV file #{csv}..."
# First get a complete list of all the query parameters
keys = Link.where(spider: true).map{|link| query_params(link).keys }.flatten.uniq.sort

# Write result to CSV file
CSV.open(csv, "w") do |csv|
  csv << ["Source URL", "Error", "Full URL", "URL (Host)", "URL (Path)", "Comparison URL"] + keys
  # For the time being don't output external urls
  Link.where(spider: true).order(:url).each do |link|
    u = URI.parse(link.url)
    params = query_params(link)
    queries = keys.map{|k| params[k]}
    csv << [link.source_url, link.error?, link.url, u.host, u.path, link.comparison_url] + queries
  end
end
