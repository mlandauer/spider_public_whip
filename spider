#!/usr/bin/env ruby

require 'mechanize'
require 'active_record'
require 'sqlite3'
require 'csv'

def get_page_links(agent, url)
  base = URI.parse(url)
  # Ignore links with nil urls
  begin
    result = agent.get(url)
  rescue
    puts "WARNING: Error retrieving page #{url}"
    return nil
  end
  # We don't want to follow any logout links
  links = result.links.delete_if{|link| link.text == "Logout"}
  links.map{|link| link.href}.compact.map do |link|
    u = base + link
    # Remove anchors 
    u.fragment = nil
    # A couple of special query parameters that we will sanitize because they result in massive duplication
    # of urls in a not particularly helpful way. Both of these parameters simply specify a return url
    u.query = u.query.split("&").delete_if do |t|
      parameter = t.split("=").first
      parameter == "r" || parameter == "rr"
    end.join("&") if u.query
    u.to_s
  end
end

def db_connect(db)
  ActiveRecord::Base.establish_connection(
    adapter:  'sqlite3', # or 'postgresql' or 'sqlite3'
    database: db
  )
end

class CreateLinks < ActiveRecord::Migration
  def self.up
    create_table :links do |t|
      t.string :url, null: false
      t.string :source_url
      # Whether to follow the links on this page
      t.boolean :spider, null: false
      t.boolean :done, null: false, default: false
      t.boolean :error, null: false, default: false
      t.timestamps
    end

    add_index :links, [:spider, :done]
    add_index :links, :url
  end
end

class Link < ActiveRecord::Base
end

db = "links.sqlite3"
csv = "links.csv"

if File.exists?(db) 
  db_connect(db)
else
  db_connect(db)
  # Create the tables we need
  CreateLinks.up
  # And put the first url in it
  Link.create!(url: "http://localhost", spider: true, done: false)
end

agent = Mechanize.new
# First login
page = agent.get("http://localhost/account/settings.php")
form = page.forms.first
form["user_name"] = "mlandauer"
form["password"] = "foofoo"
form.click_button

while !Link.where(spider: true, done: false).empty?
  link = Link.where(spider: true, done: false).first
  puts "#{link.url} (#{Link.where(spider: true, done: false).count} left)"
  new_urls = get_page_links(agent, link.url)
  if new_urls.nil?
    # There was an error reading from this url
    link.update_attributes(done: true, error: true)
  else
    ActiveRecord::Base.transaction do
      new_urls.each do |url|
        # So let's spider this one (unless we're already doing it)
        if Link.find_by(url: url).nil?
          Link.create(url: url, source_url: link.url, spider: (URI.parse(url).host == "localhost"))
        end
      end
      link.update_attributes(done: true)
    end
  end
end

# Extract the query parameters from the url
def query_params(url)
  u = URI.parse(url)
  queries = {}
  if u.query
    u.query.split("&").each do |t|
      key, value = t.split("=").map{|t| CGI::unescape(t)}
      queries[key] = value
    end
  end
  queries
end

puts "Writing CSV file #{csv}..."
# First get a complete list of all the query parameters
keys = Link.where(spider: true).map{|link| query_params(link.url).keys }.flatten.uniq.sort

# Write result to CSV file
CSV.open(csv, "w") do |csv|
  csv << ["Source URL", "Error", "URL (Host)", "URL (Path)"] + keys
  # For the time being don't output external urls
  Link.where(spider: true).order(:url).each do |link|
    u = URI.parse(link.url)
    params = query_params(link.url)
    queries = keys.map{|k| params[k]}
    csv << [link.source_url, link.error?, u.host, u.path] + queries
  end
end

